{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring LangChain: Components & RAG Walkthrough\n",
    "\n",
    "**Presenter:** Jack Tol  \n",
    "**Website:** [jacktol.net](https://jacktol.net)\n",
    "\n",
    "**Education:**  \n",
    "- Bachelor of Artificial Intelligence  \n",
    "  University of Technology Sydney  \n",
    "  *2023 - Present*\n",
    "\n",
    "**Overview:** Today, we'll delve into LangChain, exploring its key components and concluding with a walkthrough on leveraging Langchain's components and integrations to build a RAG prototype application.\n",
    "\n",
    "**Date:** 2024-06-29 \n",
    "\n",
    "**Agenda:**\n",
    "- Introduction to Langchain & Its Ecosystem\n",
    "- Reviewing a Few Langchain Components & A Brief Introduction to RAG\n",
    "- Live Textbook RAG Demonstration\n",
    "\n",
    "**Audience:** AI Enthusiasts, Developers, Researchers\n",
    "\n",
    "**Contact:** For inquiries or more information, please email [contact@jacktol.net](mailto:contact@jacktol.net) or visit [jacktol.net](https://jacktol.net).\n",
    "\n",
    "**Note:** This presentation will be recorded for future viewing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to LangChain\n",
    "\n",
    "### The LangChain Ecosystem\n",
    "- [LangChain.com](https://www.langchain.com/) is the website for LangChain, a **software company** that creates various products to aid in the development, testing, evaluation, management, and deployment of LLM-powered AI applications.\n",
    "\n",
    "- The naming scheme adopted by LangChain can be a bit confusing. Let's start by differentiating between **LangChain the Company** and **LangChain the product**.\n",
    "\n",
    "- One of the main products offered by LangChain is also called LangChain. This framework contains multiple packages and aims to be a flexible, yet abstract AI toolkit that provides various modular components which work together to create LLM-powered software/applications. The LangChain Product will be the primary focus of today's presentation.\n",
    "\n",
    "#### The LangChain Product\n",
    "\n",
    "**LangChain** is a framework consisting of several packages:\n",
    "\n",
    "- **Langchain-Core**: Base abstractions of different components and provides a way to compose them together. No 3rd Party Integrations. Lightweight.\n",
    "- **Langchain-Community**: Community Maintained 3rd party integrations for various components.\n",
    "- **Partner Packages**: Popular integrations (e.g., langchain-openai, langchain-anthropic) are separated for better support.\n",
    "- **Langchain**: Contains chains, agents, and retrieval strategies for application architecture. All components are NOT specific to any one integration, but rather generic across all integrations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](src/langchain_components.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The LangSmith Product\n",
    "- **LangSmith**: Interpretability Developer platform for debugging, testing, evaluating, and monitoring LLM applications. RAG, CoT, Agentic Behavior.\n",
    "\n",
    "#### The LangServe Product\n",
    "- **LangServe**: A package which is built using FastAPI which makes it a bit easier deploy your Langchain chains as REST APIs for production-ready applications.\n",
    "\n",
    "### The Purpose of Langchain\n",
    "- **Langchain is a software framework** designed to simplify the integration of LLMs into applications by providing modular components.\n",
    "- **You can debug, test, evaluate, and deploy LLM applications** more efficiently using these various products and modular components.\n",
    "\n",
    "Langchain's library offers a variety of \"components\" that facilitate the development of LLM-related products and software. These components include:\n",
    "\n",
    "- LLMs\n",
    "- Embedding Models\n",
    "- Vector Stores\n",
    "- Document Loaders\n",
    "- Tools (Agentic Functions)\n",
    "\n",
    "...and much more.\n",
    "\n",
    "In this presentation, we'll explore some implementations of these components. We will conclude with a demonstration of a small-scale notebook-based RAG prototype, showcasing how these elements can be integrated and function cohesively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covering a few Langchain Components & A Brief Review of RAG\n",
    "\n",
    "There are far too many components within Langchain to share them all with you here, so I'll just be covering a few components and their implementations which I feel are the most important and intuitive to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loaders\n",
    "#### PDF Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Efficient World Models with Context-Aware Tokenization\\nVincent Micheli* 1Eloi Alonso* 1Franc ¸ois Fleuret1\\nAbstract\\nScaling up deep Reinforcement Learning (RL)\\nmethods presents a significant challenge. Follow-\\ning developments in generative modelling, model-\\nbased RL positions itself as a strong contender.\\nRecent advances in sequence modelling have led\\nto effective transformer-based world models, al-\\nbeit at the price of heavy computations due to\\nthe long sequences of tokens required to accu-\\nrately simulate environments. In this work, we\\npropose ∆-IRIS, a new agent with a world model\\narchitecture composed of a discrete autoencoder\\nthat encodes stochastic deltas between time steps\\nand an autoregressive transformer that predicts\\nfuture deltas by summarizing the current state\\nof the world with continuous tokens. In the\\nCrafter benchmark, ∆-IRIS sets a new state of\\nthe art at multiple frame budgets, while being\\nan order of magnitude faster to train than pre-\\nvious attention-based approaches. We release\\nour code and models at https://github.com/\\nvmicheli/delta-iris .\\n1. Introduction\\nDeep Reinforcement Learning (RL) methods have recently\\ndelivered impressive results (Ye et al., 2021; Hafner et al.,\\n2023; Schwarzer et al., 2023) in traditional benchmarks\\n(Bellemare et al., 2013; Tassa et al., 2018). In light of the\\nevermore complex domains tackled by the latest generations\\nof generative models (Rombach et al., 2022; Achiam et al.,\\n2023), the prospect of training agents in more ambitious\\nenvironments (Kanervisto et al., 2022) may hold signifi-\\ncant appeal. However, that leap forward poses a serious\\nchallenge: deep RL architectures have been comparatively\\nsmaller and less sample-efficient than their (self-)supervised\\ncounterparts. In contrast, more intricate environments ne-\\ncessitate models with greater representational power and\\nhave higher data requirements.\\n*Equal contribution1University of Geneva, Switzerland. Corre-\\nspondence to: <first.last@unige.ch >.\\nProceedings of the 41stInternational Conference on Machine\\nLearning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\\nthe author(s).Model-based RL (MBRL) (Sutton & Barto, 2018) is hy-\\npothesized to be the key for scaling up deep RL agents\\n(LeCun, 2022). Indeed, world models (Ha & Schmidhuber,\\n2018) offer a diverse range of capabilities: lookahead search\\n(Schrittwieser et al., 2020; Ye et al., 2021), learning in imag-\\nination (Sutton, 1991; Hafner et al., 2023), representation\\nlearning (Schwarzer et al., 2021; D’Oro et al., 2023), and un-\\ncertainty estimation (Pathak et al., 2017; Sekar et al., 2020).\\nIn essence, MBRL shifts the focus from the RL problem to\\na generative modelling problem, where the development of\\nan accurate world model significantly simplifies policy train-\\ning. In particular, policies learnt in the imagination of world\\nmodels are freed from sample efficiency constraints, a com-\\nmon limitation of RL agents that is magnified in complex\\nenvironments with slow rollouts.\\nRecently, the IRIS agent (Micheli et al., 2023) achieved\\nstrong results in the Atari 100k benchmark (Bellemare et al.,\\n2013; Kaiser et al., 2020). IRIS introduced a world model\\ncomposed of a discrete autoencoder and an autoregressive\\ntransformer, casting dynamics learning as a sequence mod-\\nelling problem where the transformer composes over time a\\nvocabulary of image tokens built by the autoencoder. This\\napproach opened up avenues for future model-based meth-\\nods to capitalize on advances in generative modelling (Ville-\\ngas et al., 2022; Achiam et al., 2023), and has already been\\nadopted beyond its original domain (comma.ai, 2023; Hu\\net al., 2023). However, in its current form, scaling IRIS to\\nmore complex environments is computationally prohibitive.\\nIndeed, such an endeavor requires a large number of tokens\\nto encode visually challenging frames. Besides, sophisti-\\ncated dynamics may require to store numerous time steps\\nin memory to reason about the past, ultimately making the\\nimagination procedure excessively slow. Hence, under these\\nconstraints, maintaining a favorable imagined-to-collected\\ndata ratio is practically infeasible.\\nIn the present work, we introduce ∆-IRIS, a new agent\\ncapable of scaling to visually complex environments with\\nlengthier time horizons. ∆-IRIS encodes frames by attend-\\ning to the ongoing trajectory of observations and actions,\\neffectively describing stochastic deltas between time steps.\\nThis enriched conditioning scheme drastically reduces the\\nnumber of tokens to encode frames, offloads the determinis-\\ntic aspects of world modelling to the autoencoder, and lets\\nthe autoregressive transformer focus on stochastic dynamics.\\n1arXiv:2406.19320v1  [cs.LG]  27 Jun 2024', metadata={'source': 'example_data\\\\pdf_loader_example.pdf', 'page': 0}),\n",
       " Document(page_content='Efficient World Models with Context-Aware Tokenization\\nz1\\n1 . . .zKI\\n1z1\\n2 . . .zKI\\n2\\nx1ˆx1\\nx2ˆx2\\nx0 a0 z1\\n1 . . . zK\\n1x1 a1 z1\\n2 . . . zK\\n2\\nx1 x2 x0 a0 a1 a2ˆx1 ˆx2\\nEncoderDecoder\\nFigure 1. Discrete autoencoder of IRIS (Micheli et al., 2023) (left) and ∆-IRIS (right). IRIS encodes and decodes frames independently,\\nmeaning that zthas to carry all the information necessary to reconstruct xt. On the other hand, ∆-IRIS’ encoder and decoder are\\nconditioned on past frames and actions, thus ztonly has to capture what has changed and that cannot be inferred from actions, i.e. the\\nstochastic delta. This conditioning scheme enables us to drastically reduce the number of tokens required to encode a frame with minimal\\nloss (K≪KI), which is critical to speed up the autoregressive transformer that predicts future tokens.\\nNonetheless, substituting the sequence of absolute image\\ntokens with a sequence of ∆-tokens makes the task of the\\nautoregressive model more arduous. In order to predict the\\nnext transition, it may only reason over previous ∆-tokens,\\nand thus faces the challenge of integrating over multiple\\ntime steps as a way to form a representation of the current\\nstate of the world. To resolve this issue, we modify the\\nsequence of the autoregressive model by interleaving contin-\\nuous I-tokens, that summarize successive world states with\\nframe embeddings, and discrete ∆-tokens.\\nIn the Crafter benchmark (Hafner, 2022), ∆-IRIS exhibits\\nfavorable scaling properties: the agent solves 17 out of\\n22 tasks after 10M frames of data collection, supersedes\\nDreamerV3 (Hafner et al., 2023) at multiple frame bud-\\ngets, and trains 10 times faster than IRIS. In addition, we\\ninclude results in the sample-efficient setting with Atari\\ngames. Through experiments, we provide evidence that ∆-\\nIRIS learns to disentangle the deterministic and stochastic\\naspects of world modelling. Moreover, we conduct ab-\\nlations to validate the new conditioning schemes for the\\nautoencoder and transformer models.\\n2. Method\\nWe consider a Partially Observable Markov Decision Pro-\\ncess ( POMDP ) (Sutton & Barto, 2018). The transition,\\nreward, and episode termination dynamics are captured\\nby the conditional distributions p(xt+1|x≤t, a≤t)and\\np(rt, dt|x≤t, a≤t), where xt∈ X =R3×h×wis an im-\\nage observation, at∈ A ={1, . . . , A }a discrete action,\\nrt∈Ra scalar reward, and dt∈ {0,1}indicates episode\\ntermination. The reinforcement learning objective is to finda policy pπ(at|x≤t, a<t)that maximizes the expected sum\\nof rewards Eπ[P\\nt≥0γtrt], with discount factor γ∈(0,1).\\nLearning in imagination (Sutton, 1991; Sutton & Barto,\\n2018) consists of 3 stages that are repeated alternatively:\\nexperience collection, world model learning, and policy\\nimprovement. Strikingly, the agent learns behaviours purely\\nwithin its world model, and real experience is only leveraged\\nto learn the environment dynamics.\\nIn the vein of IRIS (Micheli et al., 2023), our world model\\nis composed of a discrete autoencoder (Van Den Oord et al.,\\n2017) and an autoregressive transformer (Vaswani et al.,\\n2017; Radford et al., 2019), albeit with new conditioning\\nschemes and architectures. We first expose IRIS’ world\\nmodel in Section 2.1, then present ∆-IRIS’ autoencoder and\\nautoregressive model in Sections 2.2 and 2.3, respectively.\\nFinally, we describe the policy improvement phase in Sec-\\ntion 2.4. Appendix A gives a detailed breakdown of model\\narchitectures and hyperparameters.\\n2.1. Background: IRIS\\nHigh-dimensional images are converted into tokens with a\\ndiscrete autoencoder (EI, DI)(Van Den Oord et al., 2017).\\nThe encoder EI:Rh×w×3→ {1, . . . , N I}KImaps an in-\\nput image xtintoKItokens from a vocabulary of size NI.\\nThe discretization is done by picking the index of the vector\\nin the vocabulary embedding table that is closest to the en-\\ncoder output yt∈RKI×d. The KItokens are then decoded\\nback into an image with DI:{1, . . . , N I}KI→Rh×w×3.\\nThis discrete autoencoder is trained with L1reconstruction,\\nperceptual (Esser et al., 2021) and commitment losses (Van\\nDen Oord et al., 2017) computed on collected frames.\\n2', metadata={'source': 'example_data\\\\pdf_loader_example.pdf', 'page': 1}),\n",
       " Document(page_content='Efficient World Models with Context-Aware Tokenization\\n˜x0a0 z1\\n1z2\\n1 . . . zK−1\\n1zK\\n1˜x1a1 z1\\n2z2\\n2 . . . zK−1\\n2zK\\n2˜x2a2 z1\\n3z2\\n3 . . . zK−1\\n3zK\\n3ˆz1\\n1ˆz2\\n1 . . . ˆzK−1\\n1ˆzK\\n1ˆz1\\n2ˆz2\\n2 . . . ˆzK−1\\n2ˆzK\\n2ˆz1\\n3ˆz2\\n3 . . . ˆzK−1\\n3ˆzK\\n3ˆr0ˆd0\\nˆr1ˆd1\\nˆr2ˆd2\\nx0 x1 x2\\nFigure 2. Unrolling dynamics over time. At each time step (separated by dashed lines), the GPT-like autoregressive transformer Gpredicts\\nthe∆-tokens for the next frame, as well as the reward and a potential episode termination. Its input sequence consists of action tokens,\\n∆-tokens, and I-tokens, namely continuous image embeddings that alleviate the need to attend to past ∆-tokens for world modelling.\\nMore specifically, an initial frame x0is embedded into I-token ˜x0. From ˜x0anda0,Gpredicts the reward ˆr0, episode termination\\nˆd0∈ {0,1}, and in an autoregressive manner ˆz1= (ˆz1\\n1, . . . , ˆzK\\n1), the∆-tokens for the next frame. Note that, during the imagination\\nprocedure, the next frame (stripped box) is computed by the decoder Dbased on previous frames, actions, and the ∆-tokens generated by\\nG, i.e.x1=D(x0, a0,ˆz1).\\nThe transformer GImodels the environment dynamics by\\noperating over an input sequence of image and action tokens\\n(z1\\n0, . . . , zKI\\n0, a0, z1\\n1, . . . , zKI\\n1, a1, . . . , z1\\nt, . . . , zKI\\nt, at).\\nImage and action tokens are embedded with learnt\\nlookup tables. At each time step, GIpredicts\\nthe transition, reward, and termination distributions:\\npGI(ˆzt+1|z≤t, a≤t)withˆzk\\nt+1∼pGI(ˆzk\\nt+1|z≤t, a≤t, z<k\\nt+1),\\npGI(ˆrt|z≤t, a≤t), and pGI(ˆdt|z≤t, a≤t). The model is\\ntrained with a cross-entropy loss on segments sampled from\\npast experience.\\nAt a high level, the autoencoder builds a vocabulary of image\\ntokens to encode each frame, and the transformer captures\\nthe environment dynamics by autoregressively composing\\nthe vocabulary over time. As a result, this world model\\nis capable of attending to previous time steps to make its\\npredictions, and models the joint law of future latent states.\\n2.2. Disentangling deterministic and stochastic\\ndynamics\\nIRIS (Micheli et al., 2023) encodes frames independently,\\nmaking no assumption about temporal redundancy within\\ntrajectories. One major drawback of this general formu-\\nlation is that, in environments with visually challenging\\nframes, a large number of tokens is required to encode\\nframes losslessly. Consequently, computations with the dy-\\nnamics model become increasingly prohibitive, as the atten-\\ntion mechanism scales quadratically with sequence length.\\nTherefore, limiting computation under such a trade-off may\\nresult in degraded performance (Micheli et al. (2023) app.\\nE)One possible solution to achieve fast world modelling with\\nminimal loss is to condition the autoencoder on previous\\nframes and actions. Intuitively, encoding a frame given pre-\\nvious frames consists in describing what has changed, the\\ndelta, between successive time steps. In many environments,\\nthe delta between frames is often much simpler to describe\\nthan the frames themselves. As a matter of fact, when\\nthe transition function is deterministic, adding previous ac-\\ntions to the conditioning of the decoder results in a world\\nmodel, without the need to encode any information between\\ntime steps. However, most environments of interest feature\\nstochastic dynamics, and apart from aleatoric uncertainty,\\narchitectural limitations such as the agent’s memory may\\ninduce additional epistemic uncertainty. Hence, the delta\\nbetween two time steps usually consists of deterministic and\\nstochastic components.\\nFor instance, an agent moving from one square to another\\nin a grid-like environment when pressing movement keys\\ncan be seen as a deterministic component of the transition.\\nOn the other hand, the sudden apparition of an enemy in\\na nearby square is a random event. Interestingly, only the\\nstochastic features of a transition should be encoded, and\\nthe autoencoder could directly learn to model the determin-\\nistic dynamics, which do not require the expressivity and\\nability to handle multimodality of an autoregressive model.\\nTherefore, when autoenconding frames by conditioning on\\nprevious frames and actions, a frame encoding may only\\nconsist of a handful of ∆-tokens, instead of a large number\\nof image tokens describing frames independently.\\nSection 3.4 provides empirical evidence that ∆-IRIS’ autoen-\\ncoder learns to encode frames in such fashion, and Figure 1\\nillustrates the new conditioning scheme of the autoencoder.\\n3', metadata={'source': 'example_data\\\\pdf_loader_example.pdf', 'page': 2}),\n",
       " Document(page_content='Efficient World Models with Context-Aware Tokenization\\n∆-tokens sampled randomly\\n∆-tokens sampled by the autoregressive transformer\\nt= 0 t= 4 t= 5 t= 9 t= 10 t= 12\\nFigure 3. Evidence of dynamics disentanglement. Two trajectories are imagined with different ways of generating ∆-tokens. In the top\\ntrajectory, ∆-tokens are sampled randomly. In the bottom trajectory, the autoregressive transformer predicts future ∆-tokens. The same\\nstarting frame ( t= 0) and sequence of actions are used. With random ∆-tokens, the deterministic aspects of the dynamics (layout,\\nmovement, items, crafting) are still properly modelled, but the stochastic dynamics (mobs, health indicators) become problematic. For\\ninstance, the agent successfully cuts down a tree between t= 4andt= 5, and uses wood planks to build a crafting table between t= 10\\nandt= 12 . We observe that these dynamics are modelled in the same way whether ∆-tokens are sampled randomly or not. However, in\\nthe top trajectory, large quantities of cows appear and disappear from the screen incoherently, whereas the bottom trajectory does not\\ndisplay such erratic patterns. This experiment shows that ∆-IRIS encodes stochastic deltas between time steps with ∆-tokens, and its\\ndecoder handles the deterministic aspects of world modelling. Appendix F contains additional examples.\\nMore formally, for any set Y, we denote Sn(Y) =Sn\\ni=1Yi\\nthe set of tuples of elements from Yof maximum length\\nn, andS(Y) =S∞(Y). LetZ={1, . . . , N }a vocab-\\nulary of discrete tokens. Given past images and actions\\n(x0, a0, . . . , x t−1, at−1), the encoder E:S(X ×A )×X →\\nZKconverts an image xtintozt= (z1\\nt, . . . , zK\\nt), a se-\\nquence of Kdiscrete ∆-tokens. The encoder is param-\\neterized by a Convolutional Neural Network ( CNN) (Le-\\nCun et al., 1989). Actions are embedded with a learnt\\nlookup table and concatenated channel-wise with frames.\\nWe use vector quantization (Van Den Oord et al., 2017;\\nEsser et al., 2021) with factorized and normalized codes\\n(Yu et al., 2021) to discretize the encoder’s continuous\\noutputs. The CNN decoder D:S(X × A )× ZK→ X\\nreconstructs an image ˆxtfrom past frames, actions and ∆-\\ntokens (x0, a0, . . . , x t−1, at−1, zt). Action and ∆-tokens\\nare embedded with learnt lookup tables, and concatenated\\nchannel-wise with feature maps obtained by forwarding\\nframes through an auxiliary CNN.\\nThe discrete autoencoder is trained on previously collected\\ntrajectories with a weighted combination of L1,L2and\\nmax-pixel (Anand et al., 2022) reconstruction losses, as\\nwell as a commitment loss (Van Den Oord et al., 2017). The\\ncodebook is updated with an exponential moving average\\n(Razavi et al., 2019) and we use a straight-through estimator\\n(Bengio et al., 2013) to enable backpropagation.2.3. Modelling stochastic dynamics\\nWhile it should be possible to predict future ∆-tokens, given\\na starting image, past actions and ∆-tokens, we found this\\ntask much more difficult than simply predicting future image\\ntokens, given past image tokens and actions, as in IRIS.\\nTo better understand why this is the case, let us consider\\nanother example: in a grid environment, ∆-tokens may\\ndescribe the unpredictable movement of an enemy, ran-\\ndomly jumping from one square to another at every time\\nstep. Based on the initial enemy location and after only a\\nfew time steps, it becomes increasingly difficult to predict\\nif the enemy and the agent are located on the same square,\\nwhich could trigger a battle and make the enemy disap-\\npear. Indeed, situating the two entities involves reasoning\\nabout the initial observation, and integrating over all of the\\nprevious action and ∆-tokens, which may have a complex\\ndependence structure.\\nTo address this problem, we alter the sequence of the dynam-\\nics model by interleaving continuous I-tokens, in reference\\nto MPEG’s I-frames (Richardson, 2004), and discrete ∆-\\ntokens. I-tokens alleviate the need of integrating over past\\n∆-tokens to form a representation of the current state of\\nthe world, i.e. they deploy a “soft” Markov blanket for the\\nprediction of the next ∆-tokens.\\n4', metadata={'source': 'example_data\\\\pdf_loader_example.pdf', 'page': 3}),\n",
       " Document(page_content='Efficient World Models with Context-Aware Tokenization\\nTable 1. Returns, number of parameters, and frames collected per second (FPS) for the methods considered. We compute FPS as the total\\nnumber of environment frames collected divided by the training duration. ∆-IRIS outperforms DreamerV3 for larger frame budgets, and\\nis 10x faster than IRIS (64 tokens).\\nMethod Return @1M Return @5M Return @10M #Parameters FPS\\n∆-IRIS 7.7 (0.5) 15.4 (0.4) 16.1 (0.1) 25M 20\\nDreamerV3 XL 9.2(0.3) 14.2 (0.2) 15.1 (0.3) 200M 30\\nIRIS (64 tokens) 5.5 (0.7) - - 48M 2\\n∆-IRIS w/o I-tokens 6.6 (0.2) 10.4 (0.5) 12.6 (0.8) 24M 22\\nDreamerV3 M 6.2 (0.5) 12.6 (0.7) 13.7 (0.8) 37M 40\\nIRIS (16 tokens) 4.4 (0.1) - - 50M 6\\n 4 6 8 10 12 14 16 18\\n1M 2M 3M 4M 5M 6M7M8M9MReturn\\nNumber of framesDreamerV3 MDreamerV3 XLIRIS (16 tokens)IRIS (64 tokens)Δ-IRIS w/o I-tokensΔ-IRIS\\nFigure 4. Returns at multiple frame budgets in the Crafter bench-\\nmark. ∆-IRIS achieves higher returns than DreamerV3 beyond\\n3M frames, and surpasses IRIS for all frame budgets considered.\\nRemoving I-tokens from the input sequence of the autoregressive\\ntransformer significantly hurts performance.\\nWe obtain I-tokens by forwarding frames through an aux-\\niliary CNN at each time step. They are not produced by a\\ndiscrete autoencoder. Since I-tokens are not predicted by the\\nmodel but rather enrich its conditioning, there are no incen-\\ntives to include a lossy discretization operator or to optimize\\na reconstruction loss. Instead, they are optimized end-to-\\nend with the learning objectives of the dynamics model.\\nWith this improved conditioning, the dynamics model per-\\nceives the ongoing trajectory with a mixture of continuous\\nand discrete representations, while making its predictions\\nautoregressively in a discrete space.\\nFigure 2 displays the input sequence of the dynam-\\nics model and the quantities it predicts. Given a se-\\nquence of past I-tokens, action tokens, and ∆-tokens\\n(˜x0, a0, z1\\n1, . . . , zK\\n1, . . . , ˜xt−1, at−1, z1\\nt, . . . , zk\\nt), the dy-\\nnamics model Goutputs a categorical distribution on Z\\nfor the next ∆-token ˆzk+1\\nt∼pG(ˆzk+1\\nt|˜x<t, z<t, a<t, z≤k\\nt).\\nIt also predicts distributions for rewards pG(ˆrt|˜x≤tz≤t, a≤t)\\nand episode terminations pG(ˆdt|˜x≤t, z≤t, a≤t).Gis parameterized by a stack of transformer encoder layers\\nwith causal self-attention (Vaswani et al., 2017; Radford\\net al., 2019). It is trained with a cross-entropy loss for transi-\\ntion and termination predictions, and we follow DreamerV3\\n(Hafner et al., 2023) in using discrete regression with two-\\nhot targets and symlog scaling for reward prediction (Imani\\n& White, 2018).\\n2.4. Policy improvement\\nDuring the policy improvement phase, the policy πlearns\\nin the imagination POMDP of its world model, composed of\\nthe autoencoder (E, D)and the dynamics model G.\\nAt time step t, the policy observes a reconstructed image\\nobservation ˆxtand samples action at∼π(at|ˆx≤t). The\\nworld model then predicts the reward ˆrt, the episode end ˆdt,\\nand the next observation ˆxt+1=D(ˆx≤t,ˆa≤t,ˆz≤t,ˆzt+1),\\nwithˆzt+1∼pG(ˆzt+1|ˆx≤t,ˆa≤t,ˆz≤t). The imagination pro-\\ncedure is initialized with a real observation x0sampled from\\npast experience, and is rolled out for Hsteps. The procedure\\nstops if an episode termination is predicted before reaching\\nthe imagination horizon.\\nWe employ to a large extent the actor-critic training method\\nused for IRIS (Micheli et al., 2023). A value baseline is\\ntrained to predict λ-returns (Sutton & Barto, 2018) with the\\nsame discrete regression objective as for reward prediction.\\nThe policy optimizes the REINFORCE with value baseline\\n(Sutton & Barto, 2018) learning objective over imagined\\ntrajectories. Exploration is encouraged by adding an entropy\\nmaximization term to the policy’s objective.\\n3. Experiments\\nIn our experiments, we consider the Crafter benchmark\\n(Hafner, 2022) to illustrate ∆-IRIS’ ability to scale to a\\nvisually rich environment with large frame budgets. Be-\\nsides, we also include Atari 100k games (Bellemare et al.,\\n2013; Kaiser et al., 2020) in Appendix C to showcase the\\nperformance and speed of our agent in the sample-efficient\\nsetting.\\n5', metadata={'source': 'example_data\\\\pdf_loader_example.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader_example = PyPDFLoader(\"example_data\\pdf_loader_example.pdf\")\n",
    "pages =  pdf_loader_example.load()\n",
    "pages[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"document_id: 0809.0182\\nauthors: [{'keyname': 'Xie', 'forenames': 'Z. Y.'}, {'keyname': 'Jiang', 'forenames': 'H. C.'}, {'keyname': 'Chen', 'forenames': 'Q. N.'}, {'keyname': 'Weng', 'forenames': 'Z. Y.'}, {'keyname': 'Xiang', 'forenames': 'T.'}]\\ntitle: Second Renormalization of Tensor-Network States\", metadata={'source': 'example_data\\\\metadata_parsed_arxiv.csv', 'row': 0}),\n",
       " Document(page_content=\"document_id: 0810.0725\\nauthors: [{'keyname': 'Shadrin', 'forenames': 'S.'}]\\ntitle: BCOV theory via Givental group action on cohomological field theories\", metadata={'source': 'example_data\\\\metadata_parsed_arxiv.csv', 'row': 1}),\n",
       " Document(page_content=\"document_id: 0909.0800\\nauthors: [{'keyname': 'Shadrin', 'forenames': 'Sergey'}, {'keyname': 'Zvonkine', 'forenames': 'Dimitri'}]\\ntitle: A group action on Losev-Manin cohomological field theories\", metadata={'source': 'example_data\\\\metadata_parsed_arxiv.csv', 'row': 2}),\n",
       " Document(page_content=\"document_id: 0911.2157\\nauthors: [{'keyname': 'Herrero', 'forenames': 'R.'}, {'keyname': 'Pi', 'forenames': 'F.'}, {'keyname': 'Rius', 'forenames': 'J.'}, {'keyname': 'Orriols', 'forenames': 'G.'}]\\ntitle: About the oscillatory possibilities of the dynamical systems\", metadata={'source': 'example_data\\\\metadata_parsed_arxiv.csv', 'row': 3}),\n",
       " Document(page_content=\"document_id: 0912.5115\\nauthors: [{'keyname': 'Buryak', 'forenames': 'A.'}, {'keyname': 'Shadrin', 'forenames': 'S.'}]\\ntitle: A new proof of Faber's intersection number conjecture\", metadata={'source': 'example_data\\\\metadata_parsed_arxiv.csv', 'row': 4})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_loader_example = CSVLoader(\"example_data\\metadata_parsed_arxiv.csv\", encoding=\"utf-8\")\n",
    "csv_data = csv_loader_example.load()\n",
    "csv_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\",\n",
    "                           temperature=0,\n",
    "                           max_tokens=1024,\n",
    "                           timeout=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-0'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m claude_response \u001b[38;5;241m=\u001b[39m \u001b[43mclaude_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the meaning of life?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m claude_response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:248\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    244\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    245\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    247\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 248\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    258\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:677\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    671\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    675\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    676\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:534\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    533\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 534\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    535\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    536\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    538\u001b[0m ]\n\u001b[0;32m    539\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:524\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    523\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 524\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m         )\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:749\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 749\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    753\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_anthropic\\chat_models.py:752\u001b[0m, in \u001b[0;36mChatAnthropic._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    748\u001b[0m     stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    749\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    750\u001b[0m     )\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generate_from_stream(stream_iter)\n\u001b[1;32m--> 752\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_output(data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anthropic\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anthropic\\resources\\messages.py:904\u001b[0m, in \u001b[0;36mMessages.create\u001b[1;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    902\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m    903\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message \u001b[38;5;241m|\u001b[39m Stream[RawMessageStreamEvent]:\n\u001b[1;32m--> 904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    910\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    911\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    912\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    913\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anthropic\\_base_client.py:1249\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1237\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1245\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1246\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1247\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1248\u001b[0m     )\n\u001b[1;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anthropic\\_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    924\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    929\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    930\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\anthropic\\_base_client.py:1029\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1026\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1028\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1029\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1032\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1033\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1037\u001b[0m )\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-3-5-0'}}"
     ]
    }
   ],
   "source": [
    "claude_response = claude_llm.invoke(\"What is the meaning of life?\")\n",
    "claude_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "arxiv_example_index = PineconeVectorStore.from_existing_index(embedding=embedding_model, index_name=\"arxiv-example-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = arxiv_example_index.similarity_search(query=\"Uniform stability of the damped wave equation with a confining potential in the Euclidean space\", k=5)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief Review of RAG\n",
    "\n",
    "- RAG Stands for Retrieval Augmented Generation.\n",
    "- A RAG Pipeline consists of a few different parts, which all work together to allow a language model to generate reliable and accurate information based on retrieved context.\n",
    "- We pass through the users query and the retrieved context dynamically to a language model by filling in a prompt template.\n",
    "- The prompt template leverages a LLMs ability to follow system level instructions when directed and the fact that we can pass through content as variables to the model.\n",
    "\n",
    "\n",
    "#### Prompt Template Example\n",
    "```\n",
    "Instructions:\n",
    "Your job is to answer the user's query using only the provided context.\n",
    "Keep your answer grounded in the facts of the provided context.\n",
    "If the context does not contain the facts needed to answer the user's query, return: \"I do not have enough information available to accurately answer the question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Query:\n",
    "{user_query}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the context actually get retrieved?\n",
    "- We take our external documents (PDFs, CSVs, TXTs etc.) and use **some heuristic** to split the text (perhaps splitting after *n* number of characters or based on the tags in a programming language if we are splitting code text).\n",
    "- If our splits are too small, say, if we decided to split on sentences, we can combine a few sentences together to create a ***chunk*** of text.\n",
    "- We take our chunks of text and pass them through a ***sentence embedding*** model, and we upload the embedding vectors to a vector database.\n",
    "- Now, when a user enters a query, if we encode that query using the same embedding model, we can then perform a ***cosine similarity*** search between all the vectors in the database with respect to the users query vector, and then retrieve the ***top-k*** most similar chunks of text.\n",
    "- If we save the retrieved context to a `context` variable, we can easily send that through to the prompt template in addition to the users query and let the model generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Pipeline Diagram\n",
    "![](src/rag_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textbook RAG Demonstration Using Langchain\n",
    "### Part 1: Loading, Processing, & Uploading Our Documents\n",
    "\n",
    "Let's first quickly take a look at the Textbook PDF we are going to be using for this example, in addition to the Pinecone Vector Store where we will be uploading our chunks to.\n",
    "\n",
    "Understanding any high-level framework always begins by looking at and understanding the imports you'll be using.\n",
    "\n",
    "Looking at our imports we see that we first import our 'Document Loader/Transformer' components, which in this case is `PyPDFLoader` & `RecurseiveCharacterSplitter` respectively.\n",
    "\n",
    "Next, we import our \"Embedding Model & Vector Store\" components, which in this case is the `HuggingFaceEmbeddings` & `PineconeVectorStore` respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup our PDF Loader, passing through the path for the textbook, and our Text Splitter, passing through our desired text splitting settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now running our PDF Loader & Splitter with the convenient `load_and_split` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pdf_loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting our chunks, we can see that each chunk is stored in a list, with a `page_content` component containing the actual text of the chunk, and also a `metadata` component which stores a dictionary of metadata for each chunk, including the name of the original source of the chunk and also the page from which it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can also inspect the length of the `chunks` list and see if it makes sense, and in this case it does, especially since we set our chunk size to 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up two variables, the `embedding_model`and our `textbook_vector_store`, these are generally set up as global variables as many different parts of your codebase will use them.\n",
    "\n",
    "We set the `embedding_model` to a `HuggingFaceEmbeddings` object, and our `textbook_vector_store` to a `PineconeVectorStore` object, using the `from_existing_index` method, in which we can pass through our embedding model, in addition to the index name, which we set to the name of our Pinecone index which we have set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings()\n",
    "textbook_vector_store = PineconeVectorStore.from_existing_index(embedding=embedding_model, index_name=\"textbook-vector-store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now parse both the text and metadata component of each chunk, parsing each of these components to their own list, taking special attention to parse the metadata as a dictionary as Pinecone expects metadata to be formatted that way as we'll see shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [chunk.page_content for chunk in chunks]\n",
    "metadatas = [{'page': chunk.metadata['page'] + 1, 'document': chunk.metadata['source']} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as we would expect, when we look within the `texts` list, we see the text component of the chunks. And ditto for the our `metadatas` list, where we see the metadata, including the page number & source file name for that particular chunk, formatted as a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(texts[:5])\n",
    "display(metadatas[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now finally at the point where we can embed and then upload our chunks. We use the `from_texts` method on our `textbook_vector_store`, passing through our `texts`, `embedding_model`, `metadatas`, and `index_name`. Suprisingly, due to the amazing power and abstraction given to us by Langchain, this one line of code embeds and uploads all 2285 chunks of our textbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_vector_store.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embedding_model,\n",
    "    metadatas=metadatas,\n",
    "    index_name=\"textbook-vector-store\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly make a function to conduct a similarity search on the vector store with respect to a user query we enter, setting the number of items to retrieve (k) to 5, printing out the retrieved context chunk and the corresponding page number and source document metadata. We use the `similarity_search` method on our `textbook_vector_store`, passing through our query and k number as parameters, loop through the results, and print the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_display(query, k=5):\n",
    "    results = textbook_vector_store.similarity_search(query=query, k=k)\n",
    "    for result in results:\n",
    "        print(f\"Text: {result.page_content}\\n\")\n",
    "        print(f\"Document: {result.metadata['document']}\\n\")\n",
    "        print(f\"Page: {int(result.metadata['page'])}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Look at that, we see the context which was retrieved, which is by default ordered by descending with respect to the cosine similarity between the chunk and the user query, in addition to the name of the source document of that chunk and the page number from which the chunk came from so we can always cross-check the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: neural networks so that they can do something useful.\n",
      "The question we need to think about ﬁrst is how our neurons can learn. We are going to\n",
      "look atsupervised learning for the next few chapters, which means that the algorithms will\n",
      "learn by example: the dataset that we learn from has the correct output values associated\n",
      "with each datapoint. At ﬁrst sight this might seem pointless, since if you already know the\n",
      "\n",
      "Document: Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf\n",
      "\n",
      "Page: 64\n",
      "\n",
      "\n",
      "Text: a well-known introduction to neural networks:\n",
      "•D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning internal representations\n",
      "by back-propagating errors. Nature, 323(99):533–536, 1986a.\n",
      "•D.E. Rumelhart, J.L. McClelland, and the PDP Research Group, editors. Parallel\n",
      "Distributed Processing . MIT Press, Cambridge, MA, 1986b.\n",
      "•R. Lippmann. An introduction to computing with neural nets. IEEE ASSP Magazine ,\n",
      "pages 4–22, 1987.\n",
      "\n",
      "Document: Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf\n",
      "\n",
      "Page: 129\n",
      "\n",
      "\n",
      "Text: to look at the neural network solution proposed by Rumelhart, Hinton, and McClelland,\n",
      "the Multi-layer Perceptron (MLP), which is still one of the most commonly used machine\n",
      "learning methods around. The MLP is one of the most common neural networks in use. It\n",
      "is often treated as a ‘black box’, in that people use it without understanding how it works,\n",
      "which often results in fairly poor results. Getting to the stage where we understand how it\n",
      "\n",
      "Document: Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf\n",
      "\n",
      "Page: 94\n",
      "\n",
      "\n",
      "Text: 3.2 NEURAL NETWORKS\n",
      "One thing that is probably fairly obvious is that one neuron isn’t that interesting. It doesn’t\n",
      "do very much, except ﬁre or not ﬁre when we give it inputs. In fact, it doesn’t even learn.\n",
      "If we feed in the same set of inputs over and over again, the output of the neuron never\n",
      "varies—it either ﬁres or does not. So to make the neuron a little more interesting we need\n",
      "to work out how to make it learn, and then we need to put sets of neurons together into\n",
      "\n",
      "Document: Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf\n",
      "\n",
      "Page: 64\n",
      "\n",
      "\n",
      "Text: In terms of learning about a set of data we have now reached the stage that neural\n",
      "networks were up to in 1969. Then, two researchers, Minsky and Papert, published a book\n",
      "called “Perceptrons.” The purpose of the book was to stimulate neural network research\n",
      "by discussing the learning capabilities of the Perceptron, and showing what the network\n",
      "could and could not learn. Unfortunately, the book had another eﬀect: it eﬀectively killed\n",
      "\n",
      "Document: Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf\n",
      "\n",
      "Page: 76\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieve_and_display(\"tell me about neural networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly, let's just put this into a function, to make it easy to load, process, & upload different documents a bit more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's do our imports and define the two global variables we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "textbook_vector_store = PineconeVectorStore.from_existing_index(embedding=embedding_model, index_name=\"textbook-vector-store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_textbook_to_vector_store():\n",
    "    pdf_loader = PyPDFLoader(\"Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=len)\n",
    "    \n",
    "    chunks = pdf_loader.load_and_split(text_splitter=text_splitter)\n",
    "    \n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    metadatas = [{'page': chunk.metadata['page'] + 1, 'document': chunk.metadata['source']} for chunk in chunks]\n",
    "    \n",
    "    textbook_vector_store.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embedding_model,\n",
    "        metadatas=metadatas,\n",
    "        index_name=\"textbook-vector-store\"\n",
    "    )\n",
    "    print(f\"Sucessfully Uploaded {len(texts)} Texts & {len(metadatas)} Metadatas to the Pinecone Textbook Vector Store.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_textbook_to_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Exercise: Try this on your own documents, notes or textbooks. Additionally, see if you can get it to work with Langchain's `PyPDFDirectoryLoader`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wonderful, we can see everything is working as expected. It's now time to move onto the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Creating the RAG Component\n",
    "\n",
    "Now, that we have gone through the process of loading, splitting, embedding and uploading our textbook to the vector database, and confirmed it's working as expected, it's time to connect this system up to a language model, so we can create our Question-Answering RAG Assistant.\n",
    "\n",
    "Like before, let's inspect our imports to make sure we understand the tools we'll be using. Our first import from the `langchain_openai` integration, where we import the `ChatOpenAI` module. Our second import is from the `langchain_core` parent module, where we specifically capture the `prompts` sub-module in which we import the `ChatPromptTemplate` class.\n",
    "\n",
    "We covered the `HuggingFaceEmbeddings` and `PineconeVectorStore` imports previously, so we won't worry about covering it here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create our prompt template. The `ChatPromptTemplate` class provides a variety of flexible prompt templates which accommodates to various situations. When using the `ChatOpenAI` module, its most common to use the `from_messages` method, which formats system messages, human messages, and AI messages in an easy-to-understand list format which OpenAI LLM's expect.\n",
    "\n",
    "We set a pretty specific system message, making sure to instruct the LLM to answer the users question only using the provided context, and if the context doesn't contain the answer, to let the user know this, instead of just hallucinating a response. We also, just pass through a `context` and `query` variable, which will get filled in with information at run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Using ONLY the provided context, answer the user's query. If the provided context doesn't contain the answer, then return 'I don't have enough information to accurately answer the question.'\"),\n",
    "    (\"system\", \"Context: {context}\"),\n",
    "    (\"human\", \"User Query: {query}\"),\n",
    "    (\"ai\", \"Answer:\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply setup our inference model, in this case setting a `llm` variable to a `ChatOpenAI()` model, passing through our model name, tempature, max tokens, and timeout. There are many, many more parameters you can pass in here to customize your model, but this simple setup covers pretty much all the essentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a mechanism to capture a user query, saving it to a variable `user_query`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = input(\"Please Enter Your Query: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we defined the embedding model and vector store variables earlier, since we are going to want this section to be independent to the last section, we will redefine them here for clarification. Its helpful to think of the earlier part as the \"Document Processing & Uploading Pipeline\" and this section as the \"Query, Retrieve, & Generate Pipeline\".\n",
    "\n",
    "Each should work independently, as we may want upload many, many more textbooks in the future, but the mechanism to query that vector store and generate a response will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings()\n",
    "textbook_vector_store = PineconeVectorStore.from_existing_index(embedding=embedding_model, index_name=\"textbook-vector-store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just like before, we'll use the `similarity_search` method on out vector store to conduct a similarity search with respect to the user query, passing through a k value of 10 instead of 5 to improve the RAG performance and reliability of the system. We will set the retrieved context to a `retrieved_contexts` variable.\n",
    "\n",
    "Inspecting the format of the retrieved context, we see its in the same format as earlier. We need to loop through all the retrieved context, parsing and extracting the page_content to a context list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='neural networks so that they can do something useful.\\nThe question we need to think about ﬁrst is how our neurons can learn. We are going to\\nlook atsupervised learning for the next few chapters, which means that the algorithms will\\nlearn by example: the dataset that we learn from has the correct output values associated\\nwith each datapoint. At ﬁrst sight this might seem pointless, since if you already know the', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 64.0}),\n",
       " Document(page_content='a well-known introduction to neural networks:\\n•D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning internal representations\\nby back-propagating errors. Nature, 323(99):533–536, 1986a.\\n•D.E. Rumelhart, J.L. McClelland, and the PDP Research Group, editors. Parallel\\nDistributed Processing . MIT Press, Cambridge, MA, 1986b.\\n•R. Lippmann. An introduction to computing with neural nets. IEEE ASSP Magazine ,\\npages 4–22, 1987.', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 129.0}),\n",
       " Document(page_content='to look at the neural network solution proposed by Rumelhart, Hinton, and McClelland,\\nthe Multi-layer Perceptron (MLP), which is still one of the most commonly used machine\\nlearning methods around. The MLP is one of the most common neural networks in use. It\\nis often treated as a ‘black box’, in that people use it without understanding how it works,\\nwhich often results in fairly poor results. Getting to the stage where we understand how it', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 94.0}),\n",
       " Document(page_content='3.2 NEURAL NETWORKS\\nOne thing that is probably fairly obvious is that one neuron isn’t that interesting. It doesn’t\\ndo very much, except ﬁre or not ﬁre when we give it inputs. In fact, it doesn’t even learn.\\nIf we feed in the same set of inputs over and over again, the output of the neuron never\\nvaries—it either ﬁres or does not. So to make the neuron a little more interesting we need\\nto work out how to make it learn, and then we need to put sets of neurons together into', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 64.0}),\n",
       " Document(page_content='In terms of learning about a set of data we have now reached the stage that neural\\nnetworks were up to in 1969. Then, two researchers, Minsky and Papert, published a book\\ncalled “Perceptrons.” The purpose of the book was to stimulate neural network research\\nby discussing the learning capabilities of the Perceptron, and showing what the network\\ncould and could not learn. Unfortunately, the book had another eﬀect: it eﬀectively killed', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 76.0}),\n",
       " Document(page_content='The Multi-layer Perceptron ■89\\n4.4 EXAMPLES OF USING THE MLP\\nThis section is intended to be practical, so you should follow the examples at a computer,\\nand add to them as you wish. The MLP is rather too complicated to enable us to work\\nthrough the weight changes as we did with the Perceptron.\\nInstead, we shall look at some demonstrations of how to make the network learn about\\nsome data. As was mentioned above, we shall look at the four types of problems that are', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 110.0}),\n",
       " Document(page_content='The example we are going to use is something very simple that you already know about,\\nthe logical OR. This obviously isn’t something that you actually need a neural network to\\nlearn about, but it does make a nice simple example. So what will our neural network look\\nlike? There are two input nodes (plus the bias input) and there will be one output. The\\ninputs and the target are given in the table on the left of Figure 3.4; the right of the ﬁgure', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 69.0}),\n",
       " Document(page_content='Books that cover the area include:\\n•Section 10.14 of R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation , 2nd\\nedition, Wiley-Interscience, New York, USA, 2001.\\n•Chapter 9 of S. Haykin. Neural Networks: A Comprehensive Foundation , 2nd edition,\\nPrentice-Hall, New Jersey, USA, 1999.\\n•Section 9.3 of B.D. Ripley. Pattern Recognition and Neural Networks . Cambridge\\nUniversity Press, Cambridge, UK, 1996.\\nPRACTICE QUESTIONS', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 324.0}),\n",
       " Document(page_content='44■Machine Learning: An Algorithmic Perspective\\nFIGURE 3.2 The Perceptron network, consisting of a set of input nodes (left) connected\\nto McCulloch and Pitts neurons using weighted connections.\\ninto the network, and how many of these input values there are (which is the dimension\\n(number of elements) in the input vector). They are almost always drawn as circles, just\\nlike neurons, which is rather confusing, so I’ve shaded them a diﬀerent colour. The neurons', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 65.0}),\n",
       " Document(page_content='Learning , 2nd edition, Springer, Berlin, Germany, 2008.\\nOther texts that provide alternative views of similar material include:\\n•Chapter 1 of R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation , 2nd\\nedition, Wiley-Interscience, New York, USA, 2001.\\n•Chapter 1 of S. Haykin. Neural Networks: A Comprehensive Foundation , 2nd edition,\\nPrentice-Hall, New Jersey, USA, 1999.', metadata={'document': 'Machine Learning - An Algorithmic Perspective, Second Edition, by Stephen Marsland.pdf', 'page': 34.0})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_contexts = textbook_vector_store.similarity_search(query=user_query, k=10)\n",
    "retrieved_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's exactly what we do here. We simply loop through each doc in `retrieved_contexts` and using a list comprehension, extract the parsed context to a new context list.\n",
    "\n",
    "Looking at the context list, this looks much cleaner now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neural networks so that they can do something useful.\\nThe question we need to think about ﬁrst is how our neurons can learn. We are going to\\nlook atsupervised learning for the next few chapters, which means that the algorithms will\\nlearn by example: the dataset that we learn from has the correct output values associated\\nwith each datapoint. At ﬁrst sight this might seem pointless, since if you already know the',\n",
       " 'a well-known introduction to neural networks:\\n•D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning internal representations\\nby back-propagating errors. Nature, 323(99):533–536, 1986a.\\n•D.E. Rumelhart, J.L. McClelland, and the PDP Research Group, editors. Parallel\\nDistributed Processing . MIT Press, Cambridge, MA, 1986b.\\n•R. Lippmann. An introduction to computing with neural nets. IEEE ASSP Magazine ,\\npages 4–22, 1987.',\n",
       " 'to look at the neural network solution proposed by Rumelhart, Hinton, and McClelland,\\nthe Multi-layer Perceptron (MLP), which is still one of the most commonly used machine\\nlearning methods around. The MLP is one of the most common neural networks in use. It\\nis often treated as a ‘black box’, in that people use it without understanding how it works,\\nwhich often results in fairly poor results. Getting to the stage where we understand how it',\n",
       " '3.2 NEURAL NETWORKS\\nOne thing that is probably fairly obvious is that one neuron isn’t that interesting. It doesn’t\\ndo very much, except ﬁre or not ﬁre when we give it inputs. In fact, it doesn’t even learn.\\nIf we feed in the same set of inputs over and over again, the output of the neuron never\\nvaries—it either ﬁres or does not. So to make the neuron a little more interesting we need\\nto work out how to make it learn, and then we need to put sets of neurons together into',\n",
       " 'In terms of learning about a set of data we have now reached the stage that neural\\nnetworks were up to in 1969. Then, two researchers, Minsky and Papert, published a book\\ncalled “Perceptrons.” The purpose of the book was to stimulate neural network research\\nby discussing the learning capabilities of the Perceptron, and showing what the network\\ncould and could not learn. Unfortunately, the book had another eﬀect: it eﬀectively killed',\n",
       " 'The Multi-layer Perceptron ■89\\n4.4 EXAMPLES OF USING THE MLP\\nThis section is intended to be practical, so you should follow the examples at a computer,\\nand add to them as you wish. The MLP is rather too complicated to enable us to work\\nthrough the weight changes as we did with the Perceptron.\\nInstead, we shall look at some demonstrations of how to make the network learn about\\nsome data. As was mentioned above, we shall look at the four types of problems that are',\n",
       " 'The example we are going to use is something very simple that you already know about,\\nthe logical OR. This obviously isn’t something that you actually need a neural network to\\nlearn about, but it does make a nice simple example. So what will our neural network look\\nlike? There are two input nodes (plus the bias input) and there will be one output. The\\ninputs and the target are given in the table on the left of Figure 3.4; the right of the ﬁgure',\n",
       " 'Books that cover the area include:\\n•Section 10.14 of R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation , 2nd\\nedition, Wiley-Interscience, New York, USA, 2001.\\n•Chapter 9 of S. Haykin. Neural Networks: A Comprehensive Foundation , 2nd edition,\\nPrentice-Hall, New Jersey, USA, 1999.\\n•Section 9.3 of B.D. Ripley. Pattern Recognition and Neural Networks . Cambridge\\nUniversity Press, Cambridge, UK, 1996.\\nPRACTICE QUESTIONS',\n",
       " '44■Machine Learning: An Algorithmic Perspective\\nFIGURE 3.2 The Perceptron network, consisting of a set of input nodes (left) connected\\nto McCulloch and Pitts neurons using weighted connections.\\ninto the network, and how many of these input values there are (which is the dimension\\n(number of elements) in the input vector). They are almost always drawn as circles, just\\nlike neurons, which is rather confusing, so I’ve shaded them a diﬀerent colour. The neurons',\n",
       " 'Learning , 2nd edition, Springer, Berlin, Germany, 2008.\\nOther texts that provide alternative views of similar material include:\\n•Chapter 1 of R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation , 2nd\\nedition, Wiley-Interscience, New York, USA, 2001.\\n•Chapter 1 of S. Haykin. Neural Networks: A Comprehensive Foundation , 2nd edition,\\nPrentice-Hall, New Jersey, USA, 1999.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = [doc.page_content for doc in retrieved_contexts]\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the user query and the context, we are ready to populate our prompt template with these values. So using the `invoke` method on our template, we pass through our `user_query` and `context` variables through to their respective key.\n",
    "\n",
    "We set this finalized prompt to the `prompt_value` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = template.invoke({\n",
    "    \"query\": user_query,\n",
    "    \"context\": context\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, that our prompt template has been populated, we are ready to pass through our prompt to the LLM using the `invoke` method on our `llm`, passing through our `prompt_value` which contains our prompt. We capture the response in the `response` variable, and print out the response.\n",
    "\n",
    "Wow! Look at that. We got the response from the LLM, we just need to parse the content of the response and then we have for all intents and purposes created a RAG Pipeline in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Neural networks are computational models inspired by the human brain. They consist of interconnected nodes, or neurons, that work together to process input data and generate outputs. A single neuron by itself isn't very interesting as it only fires or does not fire when given inputs and doesn't learn. However, when neurons are combined into networks, they can learn and perform complex tasks. One of the most common types of neural networks is the Multi-layer Perceptron (MLP), which has been widely used in machine learning. Neural networks can learn from data through supervised learning, where the dataset includes correct output values for each data point.\", response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 1209, 'total_tokens': 1333}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_d576307f90', 'finish_reason': 'stop', 'logprobs': None}, id='run-13806c8f-dd29-44e1-9d1b-5530735dddad-0', usage_metadata={'input_tokens': 1209, 'output_tokens': 124, 'total_tokens': 1333})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(prompt_value)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting out the content, we get our final response from the LLM which is now our question answered using only chunks of retrieved context which we have just uploaded a few minutes prior. Beautiful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Neural networks are computational models inspired by the human brain. They consist of interconnected nodes, or neurons, that work together to process input data and generate outputs. A single neuron by itself isn't very interesting as it only fires or does not fire when given inputs and doesn't learn. However, when neurons are combined into networks, they can learn and perform complex tasks. One of the most common types of neural networks is the Multi-layer Perceptron (MLP), which has been widely used in machine learning. Neural networks can learn from data through supervised learning, where the dataset includes correct output values for each data point.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_response = response.content\n",
    "parsed_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a bit of fun, let's inspect the prompt actually sent to the model...\n",
    "\n",
    "Oh, that's not very helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"Using ONLY the provided context, answer the user's query. If the provided context doesn't contain the answer, then return 'I don't have enough information to accurately answer the question.'\"), SystemMessage(content=\"Context: ['neural networks so that they can do something useful.\\\\nThe question we need to think about ﬁrst is how our neurons can learn. We are going to\\\\nlook atsupervised learning for the next few chapters, which means that the algorithms will\\\\nlearn by example: the dataset that we learn from has the correct output values associated\\\\nwith each datapoint. At ﬁrst sight this might seem pointless, since if you already know the', 'a well-known introduction to neural networks:\\\\n•D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning internal representations\\\\nby back-propagating errors. Nature, 323(99):533–536, 1986a.\\\\n•D.E. Rumelhart, J.L. McClelland, and the PDP Research Group, editors. Parallel\\\\nDistributed Processing . MIT Press, Cambridge, MA, 1986b.\\\\n•R. Lippmann. An introduction to computing with neural nets. IEEE ASSP Magazine ,\\\\npages 4–22, 1987.', 'to look at the neural network solution proposed by Rumelhart, Hinton, and McClelland,\\\\nthe Multi-layer Perceptron (MLP), which is still one of the most commonly used machine\\\\nlearning methods around. The MLP is one of the most common neural networks in use. It\\\\nis often treated as a ‘black box’, in that people use it without understanding how it works,\\\\nwhich often results in fairly poor results. Getting to the stage where we understand how it', '3.2 NEURAL NETWORKS\\\\nOne thing that is probably fairly obvious is that one neuron isn’t that interesting. It doesn’t\\\\ndo very much, except ﬁre or not ﬁre when we give it inputs. In fact, it doesn’t even learn.\\\\nIf we feed in the same set of inputs over and over again, the output of the neuron never\\\\nvaries—it either ﬁres or does not. So to make the neuron a little more interesting we need\\\\nto work out how to make it learn, and then we need to put sets of neurons together into', 'In terms of learning about a set of data we have now reached the stage that neural\\\\nnetworks were up to in 1969. Then, two researchers, Minsky and Papert, published a book\\\\ncalled “Perceptrons.” The purpose of the book was to stimulate neural network research\\\\nby discussing the learning capabilities of the Perceptron, and showing what the network\\\\ncould and could not learn. Unfortunately, the book had another eﬀect: it eﬀectively killed', 'The Multi-layer Perceptron ■89\\\\n4.4 EXAMPLES OF USING THE MLP\\\\nThis section is intended to be practical, so you should follow the examples at a computer,\\\\nand add to them as you wish. The MLP is rather too complicated to enable us to work\\\\nthrough the weight changes as we did with the Perceptron.\\\\nInstead, we shall look at some demonstrations of how to make the network learn about\\\\nsome data. As was mentioned above, we shall look at the four types of problems that are', 'The example we are going to use is something very simple that you already know about,\\\\nthe logical OR. This obviously isn’t something that you actually need a neural network to\\\\nlearn about, but it does make a nice simple example. So what will our neural network look\\\\nlike? There are two input nodes (plus the bias input) and there will be one output. The\\\\ninputs and the target are given in the table on the left of Figure 3.4; the right of the ﬁgure', 'Books that cover the area include:\\\\n•Section 10.14 of R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation , 2nd\\\\nedition, Wiley-Interscience, New York, USA, 2001.\\\\n•Chapter 9 of S. Haykin. Neural Networks: A Comprehensive Foundation , 2nd edition,\\\\nPrentice-Hall, New Jersey, USA, 1999.\\\\n•Section 9.3 of B.D. Ripley. Pattern Recognition and Neural Networks . Cambridge\\\\nUniversity Press, Cambridge, UK, 1996.\\\\nPRACTICE QUESTIONS', '44■Machine Learning: An Algorithmic Perspective\\\\nFIGURE 3.2 The Perceptron network, consisting of a set of input nodes (left) connected\\\\nto McCulloch and Pitts neurons using weighted connections.\\\\ninto the network, and how many of these input values there are (which is the dimension\\\\n(number of elements) in the input vector). They are almost always drawn as circles, just\\\\nlike neurons, which is rather confusing, so I’ve shaded them a diﬀerent colour. The neurons', 'Learning , 2nd edition, Springer, Berlin, Germany, 2008.\\\\nOther texts that provide alternative views of similar material include:\\\\n•Chapter 1 of R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation , 2nd\\\\nedition, Wiley-Interscience, New York, USA, 2001.\\\\n•Chapter 1 of S. Haykin. Neural Networks: A Comprehensive Foundation , 2nd edition,\\\\nPrentice-Hall, New Jersey, USA, 1999.']\"), HumanMessage(content='User Query: tell me about neural networks'), AIMessage(content='Answer:')])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse this so it looks a bit nicer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_value.messages\n",
    "\n",
    "system_instructions = messages[0].content\n",
    "context_message = messages[1].content\n",
    "user_query_message = messages[2].content\n",
    "llm_response = parsed_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM INSTRUCTIONS:\n",
      "Using ONLY the provided context, answer the user's query. If the provided context doesn't contain the answer, then return 'I don't have enough information to accurately answer the question.'\n",
      "CONTEXT SENT TO MODEL:\n",
      "Context: ['neural networks so that they can do something useful.\\nThe question we need to think about ﬁrst is how our neurons can learn. We are going to\\nlook atsupervised learning for the next few chapters, which means that the algorithms will\\nlearn by example: the dataset that we learn from has the correct output values associated\\nwith each datapoint. At ﬁrst sight this might seem pointless, since if you already know the', 'a well-known introduction to neural networks:\\n•D.E. Rumelhart, G.E. Hinton, and R.J. Williams. Learning internal representations\\nby back-propagating errors. Nature, 323(99):533–536, 1986a.\\n•D.E. Rumelhart, J.L. McClelland, and the PDP Research Group, editors. Parallel\\nDistributed Processing . MIT Press, Cambridge, MA, 1986b.\\n•R. Lippmann. An introduction to computing with neural nets. IEEE ASSP Magazine ,\\npages 4–22, 1987.', 'to look at the neural network solution proposed by Rumelhart, Hinton, and McClelland,\\nthe Multi-layer Perceptron (MLP), which is still one of the most commonly used machine\\nlearning methods around. The MLP is one of the most common neural networks in use. It\\nis often treated as a ‘black box’, in that people use it without understanding how it works,\\nwhich often results in fairly poor results. Getting to the stage where we understand how it', '3.2 NEURAL NETWORKS\\nOne thing that is probably fairly obvious is that one neuron isn’t that interesting. It doesn’t\\ndo very much, except ﬁre or not ﬁre when we give it inputs. In fact, it doesn’t even learn.\\nIf we feed in the same set of inputs over and over again, the output of the neuron never\\nvaries—it either ﬁres or does not. So to make the neuron a little more interesting we need\\nto work out how to make it learn, and then we need to put sets of neurons together into', 'In terms of learning about a set of data we have now reached the stage that neural\\nnetworks were up to in 1969. Then, two researchers, Minsky and Papert, published a book\\ncalled “Perceptrons.” The purpose of the book was to stimulate neural network research\\nby discussing the learning capabilities of the Perceptron, and showing what the network\\ncould and could not learn. Unfortunately, the book had another eﬀect: it eﬀectively killed', 'The Multi-layer Perceptron ■89\\n4.4 EXAMPLES OF USING THE MLP\\nThis section is intended to be practical, so you should follow the examples at a computer,\\nand add to them as you wish. The MLP is rather too complicated to enable us to work\\nthrough the weight changes as we did with the Perceptron.\\nInstead, we shall look at some demonstrations of how to make the network learn about\\nsome data. As was mentioned above, we shall look at the four types of problems that are', 'The example we are going to use is something very simple that you already know about,\\nthe logical OR. This obviously isn’t something that you actually need a neural network to\\nlearn about, but it does make a nice simple example. So what will our neural network look\\nlike? There are two input nodes (plus the bias input) and there will be one output. The\\ninputs and the target are given in the table on the left of Figure 3.4; the right of the ﬁgure', 'Books that cover the area include:\\n•Section 10.14 of R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation , 2nd\\nedition, Wiley-Interscience, New York, USA, 2001.\\n•Chapter 9 of S. Haykin. Neural Networks: A Comprehensive Foundation , 2nd edition,\\nPrentice-Hall, New Jersey, USA, 1999.\\n•Section 9.3 of B.D. Ripley. Pattern Recognition and Neural Networks . Cambridge\\nUniversity Press, Cambridge, UK, 1996.\\nPRACTICE QUESTIONS', '44■Machine Learning: An Algorithmic Perspective\\nFIGURE 3.2 The Perceptron network, consisting of a set of input nodes (left) connected\\nto McCulloch and Pitts neurons using weighted connections.\\ninto the network, and how many of these input values there are (which is the dimension\\n(number of elements) in the input vector). They are almost always drawn as circles, just\\nlike neurons, which is rather confusing, so I’ve shaded them a diﬀerent colour. The neurons', 'Learning , 2nd edition, Springer, Berlin, Germany, 2008.\\nOther texts that provide alternative views of similar material include:\\n•Chapter 1 of R.O. Duda, P.E. Hart, and D.G. Stork. Pattern Classiﬁcation , 2nd\\nedition, Wiley-Interscience, New York, USA, 2001.\\n•Chapter 1 of S. Haykin. Neural Networks: A Comprehensive Foundation , 2nd edition,\\nPrentice-Hall, New Jersey, USA, 1999.']\n",
      "ENTERED USER QUERY:\n",
      "User Query: tell me about neural networks\n",
      "RESPONSE FROM LLM:\n",
      "Neural networks are computational models inspired by the human brain. They consist of interconnected nodes, or neurons, that work together to process input data and generate outputs. A single neuron by itself isn't very interesting as it only fires or does not fire when given inputs and doesn't learn. However, when neurons are combined into networks, they can learn and perform complex tasks. One of the most common types of neural networks is the Multi-layer Perceptron (MLP), which has been widely used in machine learning. Neural networks can learn from data through supervised learning, where the dataset includes correct output values for each data point.\n"
     ]
    }
   ],
   "source": [
    "print(f\"SYSTEM INSTRUCTIONS:\\n{system_instructions}\")\n",
    "print(f\"CONTEXT SENT TO MODEL:\\n{context_message}\")\n",
    "print(f\"ENTERED USER QUERY:\\n{user_query_message}\")\n",
    "print(f\"RESPONSE FROM LLM:\\n{llm_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to put it in a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's do our imports and define the two global variables we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "textbook_vector_store = PineconeVectorStore.from_existing_index(embedding=embedding_model, index_name=\"textbook-vector-store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textbook_rag():\n",
    "\n",
    "    user_query = input(\"Please Enter Your Query: \")\n",
    "\n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Using ONLY the provided context, answer the user's query. If the provided context doesn't contain the answer, then return 'I don't have enough information to accurately answer the question'\"),\n",
    "        (\"system\", \"Context: {context}\"),\n",
    "        (\"human\", \"User Query: {query}\"),\n",
    "        (\"ai\", \"Answer:\")\n",
    "    ])\n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.3,\n",
    "        max_tokens=None,\n",
    "        timeout=None,\n",
    "    )\n",
    "\n",
    "    retrieved_contexts = textbook_vector_store.similarity_search(query=user_query, k=10)\n",
    "    context = [doc.page_content for doc in retrieved_contexts]\n",
    "\n",
    "    prompt_value = template.invoke({\n",
    "        \"query\": user_query,\n",
    "        \"context\": context\n",
    "    })\n",
    "\n",
    "    response = llm.invoke(prompt_value)\n",
    "    parsed_response = response.content\n",
    "    print(parsed_response)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have enough information to accurately answer the question.\n"
     ]
    }
   ],
   "source": [
    "textbook_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluding Remarks\n",
    "Perfect, we have now created a complete, albeit simple RAG implementation using only Langchain components/integrations, starting at external document loading, splitting, embedding, and uploading, and then conducting on the spot retrieval based on a user query to a GPT-4o inference model, printing out the results.\n",
    "\n",
    "Please use this notebook as a base, and in your own time, experiment by adding components, swapping out components, and creating some cool applications!\n",
    "\n",
    "Thank you for listening!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
